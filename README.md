*Авторы: Алексеева Мария, Коломейцев Даниил, Можаев Евгений, Мустафина Нина, Павлова Светлана*

# Проект поисковика

В папке `search_engine_project` находится главная функция, которая запускает наш маленький поисковик, – `project_main.py`. Последовательность
действий следующая:
1. скачивание страниц – `project_crawler.py`;
2. удаление обвязки – `project_cleaning.py`;
3. лемматизация – `project_lemmatize.py`;
4. составление обратного индекса – `project_reverse_index.py`;
5. поиск релевантных документов и ранжирование – `project_search.py`.

Время работы полного цикла зависит от глубины поиска ссылок и ограничения на количество документов в собираемой коллекции. Для глубины в три ссылки и ограничения на 1000 документов время работы на обычном ноутбуке со скоростью подключения до 30 Мбит/с составляет примерно 5 минут. На уже собранной и очищенной коллекции поиск работает в пределах 10 секунд.

Код написан для MacOS, Python 3.5. Для работы нужны модули `requests`, `BeautifulSoup`, `justext`, `NLTK` (`Corpora/stopwords` из `nltk.download()`), `sclearn`, `pandas` и морфологический анализатор [`mystem`](https://tech.yandex.ru/mystem/).

## Подробнее про каждый этап работы поисковика
1. **Краулер**. Создается список ссылок и коллекция HTML-кода со страниц этих ссылок `collection_pages`. Поиск ссылок ведется от заданной корневой ссылки – мы использовали [журнал Афиша](http://www.afisha.ru/). Модуль использует breadth-first алгоритм с регулируемой глубиной. Для каждой ссылки функция инициализирует сохранение кода страницы в отдельный файл.
2. **Удаление обвязки**. C помощью модуля `justext` производится чистка кода страниц коллекции (удаляется обвязка). Создается новый каталог с чистыми страницами `clean_collection`. Ссылка сохраняется в первой строке, далее записывается текст без обвязки. На выходе – папка с чистыми текстовыми файлами.
3. **Лемматизация**. С помощью программы `mystem` лемматизируются тексты коллекции и записываются в папку `clean_collection_lemmatized`. Mystem должен находиться в той же папке, что и основной модуль. Модуль запускает очистку для каждого файла (знаки препинания и прочие артефакты; здесь сохранилась проблема лемматизации ссылки). В `project_lemmatize.py` также находится функция лемматизации запроса.
4. **Обратный индекс**. Для коллекции текстов строится обратный индекс и записывается в текстовый файл `reverse_index_dict.txt`. Далее этот обратный индекс не используется, потому что в `sklearn` есть свой.
5. **Поиск и ранжирование**. Строится матрица показателя tf-idf текст-терм. Алгоритм вычисления tf-idf документов реализуется в пакете `sklearn`. В результате его работы имеем матрицу соответствующих значений для каждого документа и вектор запроса, преобразованный в матрицу соответствующей размерности. Этот же пакет позволяет вычислить попарную косинусную близость вектора запроса пользователя вектору каждого из документов. Документы ранжируются по этой мере, выводится список из 10 документов (и соответствующих ссылок) с наилучшей мерой, упорядоченный по степени их релевантности запросу.