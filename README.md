# Проект поисковика

`project_main.py` запускает наш маленький поисковик. Последовательность действий следующая:
1. скачивание страниц – `project_crawler.py`;
2. удаление обвязки – `project_cleaning.py`;
3. лемматизация – `project_lemmatize.py`;
4. составление обратного индекса – `project_reverse_index.py`;
5. поиск релевантных документов и ранжирование – `project_search.py`.

Время работы полного цикла зависит от глубины поиска ссылок и ограничения на количество документов в собираемой коллекции. Для глубины в три ссылки и ограничения на 1000 документов время работы на обычном ноутбуке со скоростью подключения до 30 Мбит/с составляет примерно 5 минут. На уже собранной и очищенной коллекции поиск работает в пределах 10 секунд.

Код написан под MacOS, Python 3.5. Для работы нужны модули `requests`, `BeautifulSoup`, `justext`, `NLTK` (`Corpora/stopwords` из `nltk.download()`), `sklearn`, `pandas`, `numpy` и морфологический анализатор [`mystem`](https://tech.yandex.ru/mystem/).

## Подробнее про каждый этап работы поисковика
1. **Краулер**. Создается список ссылок и коллекция HTML-кода со страниц этих ссылок `collection_pages`. Поиск ссылок ведется от заданной корневой ссылки – мы использовали [журнал Афиша](http://www.afisha.ru/). Модуль использует breadth-first алгоритм с регулируемой глубиной. Для каждой ссылки функция инициализирует сохранение кода страницы в отдельный файл.
2. **Удаление обвязки**. C помощью модуля `justext` производится чистка кода страниц коллекции (удаляется обвязка). Создается новый каталог с чистыми страницами `clean_collection`. Ссылка сохраняется в первой строке, далее записывается текст без обвязки. На выходе – папка с чистыми текстовыми файлами.
3. **Лемматизация**. Здесь происходит очистка каждого файла от знаков препинания и прочих артефактов. После этого с помощью анализатора `mystem` лемматизируются тексты коллекции и записываются в папку `clean_collection_lemmatized`. `mystem` должен находиться в той же папке, что и основной модуль. Сохранилась проблема лемматизации ссылок. В `project_lemmatize.py` также находится функция лемматизации запроса.
4. **Обратный индекс**. Для коллекции текстов строится обратный индекс и записывается в текстовый файл `reverse_index_dict.txt`. Далее этот обратный индекс не используется, потому что в `sklearn` есть свой.
5. **Поиск и ранжирование**. Строится [матрица показателя tf-idf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) для текстов коллекции. Алгоритм вычисления tf-idf реализуется в пакете `sklearn`. В результате его работы имеем матрицу значений терм-документ и вектор запроса, преобразованный в матрицу соответствующей размерности. Этот же пакет позволяет вычислить попарную косинусную близость вектора запроса пользователя и вектора каждого из документов. Документы ранжируются по косинусной близости. Выводится список из 10 документов (и соответствующих ссылок) с наилучшей мерой, упорядоченный по степени релевантности документов запросу.
